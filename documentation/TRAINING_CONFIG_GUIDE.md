# MicroWakeWord Training Configuration Guide

This guide explains the parameters found in the `training_parameters.yaml` file, which controls the training process for `microWakeWord` models.

## Main Configuration Sections

The YAML file is typically structured with the following main keys:

- `window_step_ms`: (Integer) The step size of the spectrogram window in milliseconds. Default: `10`.
- `train_dir`: (String) Path to the directory where trained models, logs, and checkpoints will be saved. Example: `"trained_models/wakeword"`.
- `features`: (List of Dictionaries) Defines the datasets to be used for training, validation, and testing.
- `training_steps`: (List of Integers) Number of training steps for each phase of training. Allows for multi-stage training with different parameters. Example: `[10000, 5000]` for two stages.
- `positive_class_weight`: (List of Floats) Weight for the positive class (wake word) in the loss function, corresponding to each training stage. Helps handle class imbalance. Example: `[1.0]`.
- `negative_class_weight`: (List of Floats) Weight for the negative class in the loss function. Example: `[20.0]`.
- `learning_rates`: (List of Floats) Learning rate for the Adam optimizer for each training stage. Example: `[0.001, 0.0005]`.
- `batch_size`: (Integer) Number of samples per training batch. Example: `128`.
- `time_mask_max_size`: (List of Integers) Maximum size of the time mask for SpecAugment, per training stage. `0` disables it.
- `time_mask_count`: (List of Integers) Number of time masks to apply for SpecAugment, per training stage.
- `freq_mask_max_size`: (List of Integers) Maximum size of the frequency mask for SpecAugment, per training stage. `0` disables it.
- `freq_mask_count`: (List of Integers) Number of frequency masks to apply for SpecAugment, per training stage.
- `eval_step_interval`: (Integer) How often to evaluate the model on the validation set during training (in steps). Example: `500`.
- `clip_duration_ms`: (Integer) Maximum length of the audio clips (in milliseconds) that the streaming model will accept and process. Example: `1500`.
- `target_minimization`: (Float) A target value for the `minimization_metric`. The training aims to get the metric below this value. Example: `0.9`.
- `minimization_metric`: (String or `null`) The metric to minimize. If `null`, only maximization is considered.
  Available metrics:
  - `"loss"`: Cross-entropy error on the validation set.
  - `"accuracy"`: Accuracy on the validation set.
  - `"recall"`: Recall on the validation set.
  - `"precision"`: Precision on the validation set.
  - `"false_positive_rate"`: False positive rate on the validation set.
  - `"false_negative_rate"`: False negative rate on the validation set.
  - `"ambient_false_positives"`: Count of false positives from the `validation_ambient` set.
  - `"ambient_false_positives_per_hour"`: Estimated false positives per hour on the `validation_ambient` set.
- `maximization_metric`: (String) The metric to maximize, especially after the `minimization_metric` target is met.
  Example: `"average_viable_recall"`. (See `train.py` for how this custom metric is calculated, typically recall at low false alarm rates).

## `features` Section Details

Each dictionary in the `features` list defines a dataset source:

- `features_dir`: (String) Path to the directory containing the preprocessed feature data (e.g., mmap files of spectrograms).
  Example: `"generated_augmented_features"` or `"negative_datasets/speech"`.
- `sampling_weight`: (Float) Weight determining how often samples are drawn from this dataset for a batch. Higher values mean more frequent sampling.
- `penalty_weight`: (Float) A multiplier for the loss when samples from this dataset are misclassified. Can be used to penalize errors on certain datasets more heavily.
- `truth`: (Boolean) `true` if this dataset contains positive samples (wake word), `false` if it contains negative samples.
- `truncation_strategy`: (String) How to handle spectrograms longer than the model's input window:
  - `"truncate_start"`: Remove frames from the beginning.
  - `"truncate_end"`: Remove frames from the end.
  - `"random"`: Choose a random segment of the required length. Useful for long negative samples.
  - `"split"`: Split the longer spectrogram into multiple, non-overlapping (or minimally overlapping) segments. Typically used for `*_ambient` validation/testing sets.
  - `"default"`: Usually implies `truncate_start` or a model-specific default.
- `type`: (String) The type of data storage. Typically `"mmap"` for features generated by `mmap_ninja`.

## Multi-Stage Training

The parameters `training_steps`, `learning_rates`, `positive_class_weight`, `negative_class_weight`, and SpecAugment parameters (`time_mask_*`, `freq_mask_*`) are lists. This allows for multi-stage training. For example:

```yaml
training_steps: [10000, 5000]
learning_rates: [0.001, 0.0005]
# ... other list-based parameters
```

This configuration would train for 10,000 steps with a learning rate of 0.001, then for an additional 5,000 steps with a learning rate of 0.0005. If other list-based parameters are shorter than `training_steps`, their last value is repeated for subsequent stages.

## SpecAugment

SpecAugment is a data augmentation technique applied to spectrograms.

- `time_mask_max_size` / `freq_mask_max_size`: Maximum number of consecutive time steps or frequency bins to mask.
- `time_mask_count` / `freq_mask_count`: Number of such masks to apply.
  Setting counts or max sizes to `0` effectively disables that part of SpecAugment.

## Choosing `minimization_metric` and `maximization_metric`

The training script uses these to select the "best" model checkpoint:

1. It prioritizes models where `minimization_metric` is less than or equal to `target_minimization`.
2. Among those, it picks the one that maximizes `maximization_metric`.
3. If no model meets the `target_minimization`, it picks the one that has the lowest `minimization_metric` achieved so far, and among those (if tied), the one that maximizes `maximization_metric`.

This allows for strategies like "achieve a false positive rate below X, then maximize recall."

This guide should help in understanding and tuning the training process for `microWakeWord`.
