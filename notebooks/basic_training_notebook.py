#!/usr/bin/env python
# coding: utf-8

# # Training a microWakeWord Model
#
# This notebook steps you through training a basic microWakeWord model. It is intended as a **starting point** for advanced users. You should use Python 3.11.
#
# **The model generated will most likely not be usable for everyday use; it may be difficult to trigger or falsely activates too frequently. You will most likely have to experiment with many different settings to obtain a decent model!**
#
# In the comment at the start of certain blocks, I note some specific settings to consider modifying.
#
# This runs on Google Colab, but is extremely slow compared to training on a local GPU. If you must use Colab, be sure to Change the runtime type to a GPU. Even then, it still slow!
#
# At the end of this notebook, you will be able to download a tflite file. To use this in ESPHome, you need to write a model manifest JSON file. See the [ESPHome documentation](https://esphome.io/components/micro_wake_word) for the details and the [model repo](https://github.com/esphome/micro-wake-word-models/tree/main/models/v2) for examples.

import os
import sys
import platform
import subprocess
import requests
import zipfile
import shutil
from pathlib import Path
from tqdm import tqdm
import scipy.io.wavfile
import numpy as np
import yaml
import json

# Conditional import for display, works in Jupyter, no-op in pure script
try:
    from IPython.display import Audio, display, FileLink
except ImportError:
    def display(*args, **kwargs): pass
    def Audio(*args, **kwargs): pass
    def FileLink(*args, **kwargs): pass # Not used in this notebook's local version

print(f"Python version: {sys.version}")
print(f"Running in directory: {os.getcwd()}")

# --- Configuration & Data Paths ---
# Define a base directory for data downloaded/generated by this notebook
# This will be created inside the 'notebooks' directory, or wherever this script is run from.
BASE_DATA_DIR_MWW_NOTEBOOK = Path("./mww_notebook_data")
BASE_DATA_DIR_MWW_NOTEBOOK.mkdir(parents=True, exist_ok=True)
print(f"Using data directory for this notebook: {BASE_DATA_DIR_MWW_NOTEBOOK.resolve().absolute()}")

PIPER_REPO_DIR_MWW = BASE_DATA_DIR_MWW_NOTEBOOK / "piper-sample-generator"
PIPER_MODEL_FILENAME_MWW = "en_US-libritts_r-medium.pt"
PIPER_MODEL_URL_MWW = f"https://github.com/rhasspy/piper-sample-generator/releases/download/v2.0.0/{PIPER_MODEL_FILENAME_MWW}"
PIPER_MODEL_DIR_MWW = PIPER_REPO_DIR_MWW / "models"
PIPER_MODEL_FILE_MWW = PIPER_MODEL_DIR_MWW / PIPER_MODEL_FILENAME_MWW

MIT_RIRS_DIR_MWW = BASE_DATA_DIR_MWW_NOTEBOOK / "mit_rirs"
AUDIOSONET_DIR_MWW = BASE_DATA_DIR_MWW_NOTEBOOK / "audioset"
AUDIOSONET_16K_DIR_MWW = BASE_DATA_DIR_MWW_NOTEBOOK / "audioset_16k"
FMA_DIR_MWW = BASE_DATA_DIR_MWW_NOTEBOOK / "fma"
FMA_16K_DIR_MWW = BASE_DATA_DIR_MWW_NOTEBOOK / "fma_16k"
NEGATIVE_DATASETS_DIR_MWW = BASE_DATA_DIR_MWW_NOTEBOOK / "negative_datasets"

GENERATED_SAMPLES_DIR_MWW = BASE_DATA_DIR_MWW_NOTEBOOK / "generated_samples"
GENERATED_AUG_FEATURES_DIR_MWW = BASE_DATA_DIR_MWW_NOTEBOOK / "generated_augmented_features"
TRAINED_MODELS_DIR_MWW = BASE_DATA_DIR_MWW_NOTEBOOK / "trained_models/wakeword"

# --- Helper Functions (Simplified from trainer notebooks) ---
def run_command_mww(command_list, description, cwd=None):
    print(f"Executing: {description} -> {' '.join(command_list)}")
    try:
        process = subprocess.run(command_list, check=True, capture_output=True, text=True, cwd=cwd)
        if process.stderr: print(f"Stderr: {process.stderr}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"ERROR: Failed {description}.\nCmd: {e.cmd}\nCode: {e.returncode}\nOutput: {e.output}\nStderr: {e.stderr}")
        return False

def download_file_mww(url, output_path, description="file"):
    output_path = Path(output_path)
    if output_path.exists():
        print(f"{description} already exists at {output_path}. Skipping.")
        return True
    print(f"Downloading {description} from {url} to {output_path}...")
    try:
        response = requests.get(url, stream=True, timeout=300)
        response.raise_for_status()
        total_size = int(response.headers.get('content-length', 0))
        with open(output_path, "wb") as f, tqdm(desc=output_path.name, total=total_size, unit="B", unit_scale=True) as bar:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk); bar.update(len(chunk))
        return True
    except Exception as e: print(f"Error downloading {url}: {e}"); return False

def extract_zip_mww(zip_path, extract_to, expected_content_name=None):
    # Simplified: Assumes if extract_to/expected_content_name exists, it's done.
    if expected_content_name and (Path(extract_to) / expected_content_name).is_dir():
        print(f"'{expected_content_name}' seems extracted in {extract_to}. Skipping.")
        return True
    if not Path(zip_path).exists(): print(f"ZIP not found: {zip_path}"); return False
    print(f"Extracting {zip_path} to {extract_to}...")
    try:
        with zipfile.ZipFile(zip_path, 'r') as z: z.extractall(extract_to)
        return True
    except Exception as e: print(f"Error extracting {zip_path}: {e}"); return False

def extract_tar_mww(tar_path, extract_to):
    # Simplified: Assumes if extract_to contains files, it might be done.
    # A more robust check would be for a specific marker file/directory.
    if any(Path(extract_to).iterdir()): # Basic check if dir not empty
         tar_extracted_folder_name = Path(tar_path).name.replace(".tar.gz", "").replace(".tar", "")
         if (Path(extract_to) / tar_extracted_folder_name).is_dir():
            print(f"'{tar_extracted_folder_name}' seems extracted in {extract_to}. Skipping.")
            return True
    if not Path(tar_path).exists(): print(f"TAR not found: {tar_path}"); return False
    print(f"Extracting {tar_path} to {extract_to}...")
    return run_command_mww(["tar", "-xf", str(tar_path), "-C", str(extract_to)], f"Extracting {Path(tar_path).name}")

# --- Data Preparation Section ---
print("\n--- Preparing External Dependencies and Data ---")

# 1. Piper Sample Generator
PIPER_REPO_DIR_MWW.mkdir(parents=True, exist_ok=True)
if not (PIPER_REPO_DIR_MWW / ".git").is_dir():
    run_command_mww(["git", "clone", "https://github.com/rhasspy/piper-sample-generator.git", str(PIPER_REPO_DIR_MWW)], "Cloning Piper Sample Generator")
PIPER_MODEL_DIR_MWW.mkdir(parents=True, exist_ok=True)
download_file_mww(PIPER_MODEL_URL_MWW, PIPER_MODEL_FILE_MWW, "Piper TTS Model")
# Ensure piper-sample-generator is in sys.path for script execution
if str(PIPER_REPO_DIR_MWW) not in sys.path:
    sys.path.append(str(PIPER_REPO_DIR_MWW))

# Install torch, torchaudio, piper-phonemize-cross if not already handled by main project setup
# For a self-contained notebook, these might be needed.
# In a proper project setup, these should be dependencies of microWakeWord or the environment.
# For now, let's assume they are available in the environment.
# run_command_mww([sys.executable, "-m", "pip", "install", "torch", "torchaudio", "piper-phonemize-cross==1.2.1"], "Install Piper dependencies")


# 2. Augmentation Data
# MIT RIR
MIT_RIRS_DIR_MWW.mkdir(parents=True, exist_ok=True)
if not list(MIT_RIRS_DIR_MWW.glob("*.wav")):
    print("Downloading MIT RIR dataset...")
    try:
        from datasets import load_dataset # Keep import local to usage
        rir_dataset = load_dataset("davidscripka/MIT_environmental_impulse_responses", split="train", streaming=True, trust_remote_code=True)
        for row in tqdm(rir_dataset, desc="Processing MIT RIR"):
            name = Path(row['audio']['path']).name
            scipy.io.wavfile.write(MIT_RIRS_DIR_MWW / name, 16000, (np.array(row['audio']['array'])*32767).astype(np.int16))
    except Exception as e: print(f"Error downloading MIT RIR: {e}")
else: print("MIT RIR data seems to exist.")

# Audioset (one part for basic notebook)
AUDIOSONET_DIR_MWW.mkdir(parents=True, exist_ok=True)
AUDIOSONET_16K_DIR_MWW.mkdir(parents=True, exist_ok=True)
audioset_fname = "bal_train00.tar" # Basic notebook uses one part
audioset_tar_path = AUDIOSONET_DIR_MWW / audioset_fname
audioset_link = f"https://huggingface.co/datasets/agkphysics/AudioSet/resolve/main/data/{audioset_fname}"
if download_file_mww(audioset_link, audioset_tar_path, "Audioset part"):
    extract_tar_mww(audioset_tar_path, AUDIOSONET_DIR_MWW)
    # Convert to 16k WAV
    try:
        from datasets import Dataset, Audio # Keep import local
        audioset_flac_files = list(AUDIOSONET_DIR_MWW.glob("**/*.flac"))
        if audioset_flac_files:
            print("Converting Audioset to 16k WAV...")
            temp_ds = Dataset.from_dict({"audio": [str(p) for p in audioset_flac_files]}).cast_column("audio", Audio(sampling_rate=16000))
            for row in tqdm(temp_ds, desc="Converting Audioset"):
                name = Path(row['audio']['path']).name.replace(".flac", ".wav")
                scipy.io.wavfile.write(AUDIOSONET_16K_DIR_MWW / name, 16000, (np.array(row['audio']['array'])*32767).astype(np.int16))
    except Exception as e: print(f"Error processing Audioset: {e}")

# FMA (xs for basic notebook)
FMA_DIR_MWW.mkdir(parents=True, exist_ok=True)
FMA_16K_DIR_MWW.mkdir(parents=True, exist_ok=True)
fma_xs_zip_fname = "fma_xs.zip"
fma_xs_zip_path = FMA_DIR_MWW / fma_xs_zip_fname
fma_xs_link = f"https://huggingface.co/datasets/mchl914/fma_xsmall/resolve/main/{fma_xs_zip_fname}"
if download_file_mww(fma_xs_link, fma_xs_zip_path, "FMA XSmall"):
    extract_zip_mww(fma_xs_zip_path, FMA_DIR_MWW, "fma_small")
    # Convert to 16k WAV
    try:
        from datasets import Dataset, Audio # Keep import local
        fma_mp3_files = list((FMA_DIR_MWW / "fma_small").glob("**/*.mp3"))
        if fma_mp3_files:
            print("Converting FMA to 16k WAV...")
            temp_ds_fma = Dataset.from_dict({"audio": [str(p) for p in fma_mp3_files]}).cast_column("audio", Audio(sampling_rate=16000))
            for row in tqdm(temp_ds_fma, desc="Converting FMA"):
                name = Path(row['audio']['path']).name.replace(".mp3", ".wav")
                scipy.io.wavfile.write(FMA_16K_DIR_MWW / name, 16000, (np.array(row['audio']['array'])*32767).astype(np.int16))
    except Exception as e: print(f"Error processing FMA: {e}")


# Negative Datasets (pre-generated features)
NEGATIVE_DATASETS_DIR_MWW.mkdir(parents=True, exist_ok=True)
neg_link_root = "https://huggingface.co/datasets/kahrendt/microwakeword/resolve/main/"
neg_filenames = ['dinner_party.zip', 'dinner_party_eval.zip', 'no_speech.zip', 'speech.zip']
for fname in neg_filenames:
    zip_path = NEGATIVE_DATASETS_DIR_MWW / fname
    if download_file_mww(neg_link_root + fname, zip_path, f"Negative dataset {fname}"):
        extract_zip_mww(zip_path, NEGATIVE_DATASETS_DIR_MWW, fname.replace(".zip", ""))

print("--- Data Preparation Section Finished ---")


# Generates 1 sample of the target word for manual verification.
target_word = 'khum_puter'  # Phonetic spellings may produce better samples
print(f"\n--- Generating Test Sample for '{target_word}' ---")
GENERATED_SAMPLES_DIR_MWW.mkdir(parents=True, exist_ok=True) # Ensure output dir exists
piper_generate_script = PIPER_REPO_DIR_MWW / "generate_samples.py"

cmd_test_sample_mww = [sys.executable, str(piper_generate_script), target_word,
                     "--max-samples", "1", "--batch-size", "1",
                     "--output-dir", str(GENERATED_SAMPLES_DIR_MWW)]
run_command_mww(cmd_test_sample_mww, "Generating test audio sample")
test_audio_path = GENERATED_SAMPLES_DIR_MWW / "0.wav"
if test_audio_path.exists():
    display(Audio(str(test_audio_path), autoplay=True))
else:
    print(f"Test audio file not found at {test_audio_path}")


# Generates a larger amount of wake word samples.
print(f"\n--- Generating Wake Word Samples for '{target_word}' ---")
cmd_ww_samples_mww = [sys.executable, str(piper_generate_script), target_word,
                    "--max-samples", "1000", "--batch-size", "100", # As per original notebook
                    "--output-dir", str(GENERATED_SAMPLES_DIR_MWW)]
run_command_mww(cmd_ww_samples_mww, "Generating wake word samples")


# Sets up the augmentations.
print("\n--- Setting up Augmentations ---")
from microwakeword.audio.augmentation import Augmentation
from microwakeword.audio.clips import Clips
from microwakeword.audio.spectrograms import SpectrogramGeneration

clips_mww = Clips(input_directory=str(GENERATED_SAMPLES_DIR_MWW),
              file_pattern='*.wav', max_clip_duration_s=None, remove_silence=False,
              random_split_seed=10, split_count=0.1)
augmenter_mww = Augmentation(augmentation_duration_s=3.2,
                         augmentation_probabilities = {
                                "SevenBandParametricEQ": 0.1, "TanhDistortion": 0.1, "PitchShift": 0.1,
                                "BandStopFilter": 0.1, "AddColorNoise": 0.1, "AddBackgroundNoise": 0.75,
                                "Gain": 1.0, "RIR": 0.5,
                            },
                         impulse_paths = [str(MIT_RIRS_DIR_MWW)],
                         background_paths = [str(FMA_16K_DIR_MWW), str(AUDIOSONET_16K_DIR_MWW)],
                         background_min_snr_db = -5, background_max_snr_db = 10,
                         min_jitter_s = 0.195, max_jitter_s = 0.205)


# Augment a random clip and play it back
print("\n--- Augmenting and Playing Test Clip ---")
from microwakeword.audio.audio_utils import save_clip
augmented_clip_path_mww = BASE_DATA_DIR_MWW_NOTEBOOK / "augmented_clip.wav"
random_clip_mww = clips_mww.get_random_clip()
augmented_clip_mww = augmenter_mww.augment_clip(random_clip_mww)
save_clip(augmented_clip_mww, str(augmented_clip_path_mww))
display(Audio(str(augmented_clip_path_mww), autoplay=True))


# Augment samples and save the training, validation, and testing sets.
print("\n--- Generating Augmented Features ---")
from mmap_ninja.ragged import RaggedMmap
GENERATED_AUG_FEATURES_DIR_MWW.mkdir(parents=True, exist_ok=True)
splits_mww = ["training", "validation", "testing"]
for split_val in splits_mww:
  out_dir_split_mww = GENERATED_AUG_FEATURES_DIR_MWW / split_val
  out_dir_split_mww.mkdir(parents=True, exist_ok=True)

  split_name_mww = "train"; repetition_mww = 2
  spectrograms_mww = SpectrogramGeneration(clips=clips_mww, augmenter=augmenter_mww, slide_frames=10, step_ms=10)
  if split_val == "validation": split_name_mww = "validation"; repetition_mww = 1
  elif split_val == "testing":
    split_name_mww = "test"; repetition_mww = 1
    spectrograms_mww = SpectrogramGeneration(clips=clips_mww, augmenter=augmenter_mww, slide_frames=1, step_ms=10)
  
  print(f"Generating features for {split_name_mww} set...")
  RaggedMmap.from_generator(
      out_dir=str(out_dir_split_mww / 'wakeword_mmap'),
      sample_generator=spectrograms_mww.spectrogram_generator(split=split_name_mww, repeat=repetition_mww),
      batch_size=100, verbose=True,
  )


# Save a yaml config that controls the training process
print("\n--- Preparing Training Configuration ---")
config_yaml_mww = {}
config_yaml_mww["window_step_ms"] = 10
config_yaml_mww["train_dir"] = str(TRAINED_MODELS_DIR_MWW)
TRAINED_MODELS_DIR_MWW.mkdir(parents=True, exist_ok=True)

config_yaml_mww["features"] = [
    {"features_dir": str(GENERATED_AUG_FEATURES_DIR_MWW), "sampling_weight": 2.0, "penalty_weight": 1.0, "truth": True, "truncation_strategy": "truncate_start", "type": "mmap"},
    {"features_dir": str(NEGATIVE_DATASETS_DIR_MWW / "speech"), "sampling_weight": 10.0, "penalty_weight": 1.0, "truth": False, "truncation_strategy": "random", "type": "mmap"},
    {"features_dir": str(NEGATIVE_DATASETS_DIR_MWW / "dinner_party"), "sampling_weight": 10.0, "penalty_weight": 1.0, "truth": False, "truncation_strategy": "random", "type": "mmap"},
    {"features_dir": str(NEGATIVE_DATASETS_DIR_MWW / "no_speech"), "sampling_weight": 5.0, "penalty_weight": 1.0, "truth": False, "truncation_strategy": "random", "type": "mmap"},
    {"features_dir": str(NEGATIVE_DATASETS_DIR_MWW / "dinner_party_eval"), "sampling_weight": 0.0, "penalty_weight": 1.0, "truth": False, "truncation_strategy": "split", "type": "mmap"},
]
config_yaml_mww["training_steps"] = [10000]
config_yaml_mww["positive_class_weight"] = [1]
config_yaml_mww["negative_class_weight"] = [20]
config_yaml_mww["learning_rates"] = [0.001]
config_yaml_mww["batch_size"] = 128
config_yaml_mww["time_mask_max_size"] = [0]; config_yaml_mww["time_mask_count"] = [0]
config_yaml_mww["freq_mask_max_size"] = [0]; config_yaml_mww["freq_mask_count"] = [0]
config_yaml_mww["eval_step_interval"] = 500
config_yaml_mww["clip_duration_ms"] = 1500
config_yaml_mww["target_minimization"] = 0.9
config_yaml_mww["minimization_metric"] = None
config_yaml_mww["maximization_metric"] = "average_viable_recall"

training_params_yaml_path_mww = BASE_DATA_DIR_MWW_NOTEBOOK / "training_parameters.yaml"
with open(training_params_yaml_path_mww, "w") as file_yaml_out:
    yaml.dump(config_yaml_mww, file_yaml_out)
print(f"Training parameters saved to {training_params_yaml_path_mww}")


# Trains a model.
print("\n--- Starting Model Training ---")
# Note: LD_LIBRARY_PATH usually not needed if TF is installed correctly in the environment.
# os.environ['LD_LIBRARY_PATH'] = "/usr/lib/x86_64-linux-gnu:" + os.environ.get('LD_LIBRARY_PATH', '')

cmd_train_mww = [
    sys.executable, "-m", "microwakeword.model_train_eval",
    "--training_config", str(training_params_yaml_path_mww),
    "--train", "1", "--restore_checkpoint", "1",
    "--test_tf_nonstreaming", "0", "--test_tflite_nonstreaming", "0",
    "--test_tflite_nonstreaming_quantized", "0", "--test_tflite_streaming", "0",
    "--test_tflite_streaming_quantized", "1", "--use_weights", "best_weights",
    "mixednet", # Default model type
    "--pointwise_filters", "64,64,64,64", "--repeat_in_block", "1,1,1,1",
    "--mixconv_kernel_sizes", "[5], [7,11], [9,15], [23]",
    "--residual_connection", "0,0,0,0", "--first_conv_filters", "32",
    "--first_conv_kernel_size", "5", "--stride", "3"
]
run_command_mww(cmd_train_mww, "Training microWakeWord model")


# Output model file.
print("\n--- Model Training Finished ---")
output_tflite_path_mww = TRAINED_MODELS_DIR_MWW / "tflite_stream_state_internal_quant/stream_state_internal_quant.tflite"
if output_tflite_path_mww.exists():
    print(f"Trained model available at: {output_tflite_path_mww.resolve().absolute()}")
    # For local execution, user can find the file directly.
    # If in Colab, files.download() would be used, but this script is for local/Docker.
    # Example: shutil.copy(output_tflite_path_mww, "./final_model.tflite")
else:
    print(f"ERROR: Trained model not found at {output_tflite_path_mww}")

print("\n--- Notebook Finished ---")
